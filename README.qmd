---
title: "Activity 12: Statistical reasoning 4: prediction and evaluation"
subtitle: Feb. 16th, 2026, Calvin Munson
format: gfm
execute:
  warning: false
editor: source
---

Welcome! This is the fourth statistical reasoning activity. The goals of this activity are to understand how to evaluate models and use models to make predictions. Specifically, you will:

1.    Run and interpret multiple models on the same dataset and evaluate them to see which is best supported using WAIC and PSIS.
2.    Use model output to predict how the system will behave with new data.

------------------------------------------------------------------------

You will submit one output for this activity:

1.  A **PDF** of a rendered Quarto document with all of your R code. Please create a new Quarto document (e.g. don't use this `README.qmd`) and include all of the code that appears in this document, your own code, and **answers to all of the questions** in the "Q#" sections. Submit this PDF through Gradescope.

A reminder: **Please label the code** in your final submission in two ways: 

1. denote your answers to each question using headers that correspond to the question you're answering, and 
2. thoroughly "comment" your code: remember, this means annotating your code directly by typing descriptions of what each line does after a `#`. This will help future you!

------------------------------------------------------------------------

Let's start by reading in the relevant packages

```{r}
library(brms) # for statistics
library(tidyverse) # for data wrangling
library(lterdatasampler)
```


We are going to work with the fiddler crab and latitude data again:

```{r}
pie_crab <- lterdatasampler::pie_crab
```


# 1. Model comparison

It is common in the field of ecology to have multiple candidate models of how a system works. How do we know which is "best"? In this activity we will learn two metrics that can help: the Watanabeâ€“Akaike information criterion (**WAIC**) and Pareto Smoothed Importance Sampling (**PSIS**). 

Both metrics tell us how well the model will predict data it wasn't trained on, which is important for thinking about how well the model might predict new data (as in the next section!).

---------
## 1.1 Run and interpret two different multiple regressions

Let's remind ourselves of the columns in the crab data: 

```{r}
colnames(pie_crab)
```

We have multiple variables that may be relevant here. Air and water temperature data, plus the standard devaitions of each (representing variability and perhaps seasonality). Let's run two regressions and compare them: let's compare a model with latitude and mean water temp, and another with mean air temp. Since these are intertidal estuarine sites, either (or both) may be important.

### size ~ latitude + WATER temp

```{r}
# latitude and water model
m.crab.lat.water <- 
  brm(data = pie_crab, # Give the model the pie_crab data
      # Choose a gaussian (normal) distribution
      family = gaussian,
      # Specify the model here. 
      size ~ latitude + water_temp,
      # Here's where you specify parameters for executing the Markov chains
      # We're using similar to the defaults, except we set cores to 4 so the analysis runs faster than the default of 1
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      # Setting the "seed" determines which random numbers will get sampled.
      # In this case, it makes the randomness of the Markov chain runs reproducible 
      # (so that both of us get the exact same results when running the model)
      seed = 4,
      # Save the fitted model object as output - helpful for reloading in the output later
      file = "temporary/m.crab.lat.water")
```


```{r}
summary(m.crab.lat.water)
```



### size ~ latitude + AIR temp

```{r}
# latitude and air model
m.crab.lat.air <- 
  brm(data = pie_crab, # Give the model the pie_crab data
      # Choose a gaussian (normal) distribution
      family = gaussian,
      # Specify the model here. 
      size ~ latitude + air_temp,
      # Here's where you specify parameters for executing the Markov chains
      # We're using similar to the defaults, except we set cores to 4 so the analysis runs faster than the default of 1
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      # Setting the "seed" determines which random numbers will get sampled.
      # In this case, it makes the randomness of the Markov chain runs reproducible 
      # (so that both of us get the exact same results when running the model)
      seed = 4,
      # Save the fitted model object as output - helpful for reloading in the output later
      file = "temporary/m.crab.lat.air")
```


```{r}
summary(m.crab.lat.air)
```



### size ~ latitude + AIR AND WATER temp

```{r}
# latitude and air model
m.crab.lat.air.water <- 
  brm(data = pie_crab, # Give the model the pie_crab data
      # Choose a gaussian (normal) distribution
      family = gaussian,
      # Specify the model here. 
      size ~ latitude + air_temp + water_temp,
      # Here's where you specify parameters for executing the Markov chains
      # We're using similar to the defaults, except we set cores to 4 so the analysis runs faster than the default of 1
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      # Setting the "seed" determines which random numbers will get sampled.
      # In this case, it makes the randomness of the Markov chain runs reproducible 
      # (so that both of us get the exact same results when running the model)
      seed = 4,
      # Save the fitted model object as output - helpful for reloading in the output later
      file = "temporary/m.crab.lat.air.water")
```


```{r}
summary(m.crab.lat.air.water)
```
WAIC and PSIS functions

```{r}
# WAIC for each model
waic(m.crab.lat.air)
waic(m.crab.lat.water)
waic(m.crab.lat.air.water)

# PSIS for each model
loo(m.crab.lat.air)
loo(m.crab.lat.water)
loo(m.crab.lat.air.water)
```




--------

# 2. Predictions

--------

# 3. DIY section

--------

### Render to PDF

When you have finished, remember to pull, stage, commit, and push with GitHub:

-   Pull to check for updates to the remote branch
-   Stage your edits (after saving your document!) by checking the documents you'd like to push
-   Commit your changes with a commit message
-   Push your changes to the remote branch

Then submit the well-labeled PDF on Gradescope. Thanks!
